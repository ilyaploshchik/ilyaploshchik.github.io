{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0613fe122287fdb1a4092b1ec324ab5e18de9ec977608057b646301948d1df577",
   "display_name": "Python 3.9.4 64-bit (virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# One versus all MNIST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "# Import libraries\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.metrics import accuracy_score, f1_score\r\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\r\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "# read train data csv and save output as numpy array\r\n",
    "df_train = np.loadtxt('mnist_train.csv', delimiter=',', skiprows=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "# read validation data csv and save output as numpy array\r\n",
    "df_val = np.loadtxt('mnist_test.csv', delimiter=',', skiprows=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "# Alocating 1st 10000 raws for training to reduce computational Time\r\n",
    "df_train_red = df_train[:10000, :]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "# define features and label data\r\n",
    "X = df_train_red[:, 1:]\r\n",
    "y = df_train_red[:, 0]\r\n",
    "X_val  = df_val[:, 1:]\r\n",
    "y_val  = df_val[:, 0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "np.unique(y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "# Loaded dataset shapes\r\n",
    "\r\n",
    "print(f'Training set: X= {X.shape}, y= {y.shape}')\r\n",
    "print(f'Validation set: X= {X_val.shape}, y= {y_val.shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training set: X= (10000, 784), y= (10000,)\n",
      "Validation set: X= (10000, 784), y= (10000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM fitting and hyperparmeters tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "parameters = {'C': [1, 2],\r\n",
    "            'gamma' : [2e-07, 3e-07, 4e-07, 5e-07, 6e-07, 7e-07]\r\n",
    "            }\r\n",
    "svc = SVC()\r\n",
    "clf = GridSearchCV(svc, parameters, verbose=1, cv=2, scoring='f1_macro')\r\n",
    "\r\n",
    "clf.fit(X, y.ravel())\r\n",
    "print(f'The most optimal value for Gaussian kernel hyperparameter C at {clf.best_params_.get(\"C\")} and gamma at {clf.best_params_.get(\"gamma\")} gives an accuracy score of {clf.best_score_}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "The most optimal value for Gaussian kernel hyperparameter C at 2 and gamma at 5e-07 gives an accuracy score of 0.9607404654443994\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "svc = SVC(C=2, gamma=5e-07)\r\n",
    "svc.fit(X, y)\r\n",
    "\r\n",
    "svc_test_predict = svc.predict(X_val)\r\n",
    "\r\n",
    "# Evaluate the model\r\n",
    "print('Model performance on a validation set:')\r\n",
    "score = accuracy_score(y_val, svc_test_predict)\r\n",
    "print(f'Testing score: {score:.2%}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model performance on a validation set:\n",
      "Testing score: 96.94%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Provided score of 96,94% is higher then required by the assignemnt as of 95%. Further optimizaiton is needed to achieve even better results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One-vs-All SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "# using proposal from the following code:\r\n",
    "# https://houxianxu.github.io/implementation/One-vs-All-LogisticRegression.html"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "# Define numebr of classes\r\n",
    "n_classes =  len(np.unique(y))\r\n",
    "\r\n",
    "classifiers = []\r\n",
    "\r\n",
    "# Define one-vs-rest classifiers\r\n",
    "for i in range(n_classes):\r\n",
    "    y_train = y.copy()\r\n",
    "    index_i = (y_train == i)\r\n",
    "    y_train[index_i] = 1\r\n",
    "    y_train[~index_i] = 0\r\n",
    "    svc = SVC(C=2, gamma=5e-07, probability=True) # activate probablity argumnet for future probability prediction\r\n",
    "    svc.fit(X, y_train)\r\n",
    "    classifiers.append(svc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "# Function for predicting values\r\n",
    "\r\n",
    "def prediction(X):\r\n",
    "    pred_y = []\r\n",
    "    scores = []\r\n",
    "    for i in range(n_classes):\r\n",
    "        classifier = classifiers[i]\r\n",
    "        scores.append(classifier.predict_proba(X)[:,1])\r\n",
    "    y_pred = np.argmax(scores, axis=0)\r\n",
    "    return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "# Evaluate the model for one_vs_all\r\n",
    "y_pred = prediction(X_val)\r\n",
    "\r\n",
    "print('Model performance on a validation set using one vs all binarization scheme:')\r\n",
    "score = accuracy_score(y_val, y_pred)\r\n",
    "print(f'Testing score: {score:.2%}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model performance on a validation set using one vs all binarization scheme:\n",
      "Testing score: 97.13%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We were able to get higher score comparing to a default one vs one binarization scheme"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion matrix comparison"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "print('Confusion matrix for one vs all binarization scheme')\r\n",
    "print(confusion_matrix(y_true=y_val, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion matrix for one vs all binarization scheme\n",
      "[[ 967    0    3    0    0    1    5    1    3    0]\n",
      " [   0 1124    2    3    0    1    4    0    1    0]\n",
      " [   5    0  998    7    1    0    2    8   10    1]\n",
      " [   0    0    7  978    0    6    0    6   10    3]\n",
      " [   1    0    3    0  953    0    7    2    3   13]\n",
      " [   3    0    0   10    0  865    8    1    4    1]\n",
      " [   6    3    0    0    5    3  937    0    4    0]\n",
      " [   0    9   16    2    3    0    0  989    0    9]\n",
      " [   2    0    2    7    2    3    2    5  950    1]\n",
      " [   5    3    3    6   13    4    2    9   12  952]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "print('Confusion matrix for one vs one binarization scheme')\r\n",
    "print(confusion_matrix(y_true=y_val, y_pred=svc_test_predict))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion matrix for one vs one binarization scheme\n",
      "[[ 967    0    3    0    0    2    5    1    2    0]\n",
      " [   0 1124    3    2    0    1    3    0    2    0]\n",
      " [   5    0 1004    2    1    0    4   10    6    0]\n",
      " [   0    0   10  976    0    5    0    9    9    1]\n",
      " [   1    0    3    0  956    0    8    1    2   11]\n",
      " [   3    0    3   12    2  859    6    1    5    1]\n",
      " [   7    3    1    0    4    3  938    0    2    0]\n",
      " [   0    6   20    2    7    0    0  980    2   11]\n",
      " [   5    0    2   11    5    5    0    3  938    5]\n",
      " [   5    6    3    6   18    3    0    6   10  952]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By studing confusion matrixes I cannot identify any apparent difference between the two methods in terms of misclassifications.Both methods delivert quite similar results in terms of prediction without any clear difference."
   ],
   "metadata": {}
  }
 ]
}